{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: c:\\Users\\Clarice Shim\\Desktop\\COS30049 Computing Technology Innovation project\\COS30047_Session7_Group4\\SUBMISSION\\Models\\CNN\n",
      "TF: 2.19.0 | Keras: 3.11.3\n",
      "DATA: C:\\Users\\Clarice Shim\\Desktop\\COS30049 Computing Technology Innovation project\\COS30047_Session7_Group4\\SUBMISSION\\Final Data\n",
      "Model path exists: C:\\Users\\Clarice Shim\\Desktop\\COS30049 Computing Technology Innovation project\\COS30047_Session7_Group4\\SUBMISSION\\Models\\CNN\\models\\cnn\\cnn_best.keras True\n",
      "Will write to: {'REPORT': 'C:\\\\Users\\\\Clarice Shim\\\\Desktop\\\\COS30049 Computing Technology Innovation project\\\\COS30047_Session7_Group4\\\\SUBMISSION\\\\Models\\\\CNN\\\\reports\\\\cnn', 'PRED': 'C:\\\\Users\\\\Clarice Shim\\\\Desktop\\\\COS30049 Computing Technology Innovation project\\\\COS30047_Session7_Group4\\\\SUBMISSION\\\\Models\\\\CNN\\\\preds\\\\cnn', 'EMB': 'C:\\\\Users\\\\Clarice Shim\\\\Desktop\\\\COS30049 Computing Technology Innovation project\\\\COS30047_Session7_Group4\\\\SUBMISSION\\\\Models\\\\CNN\\\\embeddings\\\\cnn'}\n"
     ]
    }
   ],
   "source": [
    "# author : Clarice Shim\n",
    "# 00_setup (eval & export)\n",
    "\n",
    "import os, random, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# quieter TF logs + reproducibility\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# ---- Paths ----\n",
    "MODEL_NAME = \"cnn\"\n",
    "\n",
    "# Notebook CWD = <repo>/Models/CNN\n",
    "# CSVs live     = <repo>/Final Data/*.csv\n",
    "DATA   = Path(\"../..\") / \"Final Data\"\n",
    "\n",
    "# Outputs stay inside Models/CNN/\n",
    "MODELS = Path(\"models\") / MODEL_NAME        # Models/CNN/models/cnn/...\n",
    "REPORT = Path(\"reports\") / MODEL_NAME       # Models/CNN/reports/cnn/...\n",
    "PRED   = Path(\"preds\") / MODEL_NAME         # Models/CNN/preds/cnn/...\n",
    "EMB    = Path(\"embeddings\") / MODEL_NAME    # Models/CNN/embeddings/cnn/...\n",
    "\n",
    "for p in [MODELS, REPORT, PRED, EMB]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sanity checks (eval needs val/test + trained model)\n",
    "assert (DATA / \"val.csv\").exists() and (DATA / \"test.csv\").exists(), \"Missing val/test CSVs. Fix DATA path.\"\n",
    "assert (MODELS / \"cnn_best.keras\").exists(), \"Missing models/cnn/cnn_best.keras. Run training first.\"\n",
    "\n",
    "# ---- TensorFlow imports ----\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# (optional) force CPU if needed\n",
    "try:\n",
    "    tf.config.set_visible_devices([], \"GPU\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"TF:\", tf.__version__, \"| Keras:\", keras.__version__)\n",
    "print(\"DATA:\", (DATA).resolve())\n",
    "print(\"Model path exists:\", (MODELS / \"cnn_best.keras\").resolve(), (MODELS / \"cnn_best.keras\").exists())\n",
    "print(\"Will write to:\", {\n",
    "    \"REPORT\": str(REPORT.resolve()),\n",
    "    \"PRED\":   str(PRED.resolve()),\n",
    "    \"EMB\":    str(EMB.resolve()),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e796d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vectorizer (SavedModel dir).\n"
     ]
    }
   ],
   "source": [
    "# 01_load_artifacts — model, vectorizer, uniform vectorize() helper\n",
    "\n",
    "# Load best model\n",
    "model = keras.models.load_model(MODELS / \"cnn_best.keras\")\n",
    "\n",
    "# Try SavedModel dir → .keras file → rebuild from vocab.txt\n",
    "vec = None\n",
    "vec_dir  = MODELS / \"text_vectorizer_model\"\n",
    "vec_file = MODELS / \"text_vectorizer_model.keras\"\n",
    "vocab_txt = MODELS / \"vocab.txt\"\n",
    "\n",
    "if vec_dir.exists():\n",
    "    vec = tf.saved_model.load(str(vec_dir))\n",
    "    print(\"Loaded vectorizer (SavedModel dir).\")\n",
    "elif vec_file.exists():\n",
    "    vec = keras.models.load_model(vec_file)\n",
    "    print(\"Loaded vectorizer (.keras file).\")\n",
    "else:\n",
    "    assert vocab_txt.exists(), \"No vectorizer found; vocab.txt is also missing.\"\n",
    "    # Rebuild from vocab (+ seq_len from metadata if present)\n",
    "    seq_len = 300\n",
    "    meta_fp = MODELS / \"metadata.json\"\n",
    "    if meta_fp.exists():\n",
    "        try:\n",
    "            seq_len = int(json.loads(meta_fp.read_text(encoding=\"utf-8\")).get(\"seq_len\", 300))\n",
    "        except Exception:\n",
    "            pass\n",
    "    tv = keras.layers.TextVectorization(\n",
    "        max_tokens=None,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=seq_len,\n",
    "        standardize=\"lower_and_strip_punctuation\",\n",
    "    )\n",
    "    vocab = vocab_txt.read_text(encoding=\"utf-8\").splitlines()\n",
    "    tv.set_vocabulary(vocab)\n",
    "    # Wrap: (batch,1) strings -> ids\n",
    "    string_in = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    squeeze   = keras.layers.Lambda(lambda t: tf.squeeze(t, axis=1))(string_in)\n",
    "    ids_out   = tv(squeeze)\n",
    "    vec = keras.Model(string_in, ids_out)\n",
    "    print(\"Rebuilt vectorizer from vocab.txt\")\n",
    "\n",
    "def vectorize_strings(batch_strings):\n",
    "    \"\"\"Uniformly call the vectorizer, regardless of SavedModel/Keras model.\"\"\"\n",
    "    x = tf.convert_to_tensor(batch_strings)\n",
    "    x = tf.expand_dims(x, axis=1)  # (batch,1) strings\n",
    "    if hasattr(vec, \"signatures\"):  # SavedModel dir\n",
    "        out = vec.signatures[\"serving_default\"](x)\n",
    "        return list(out.values())[0]\n",
    "    return vec(x)                   # Keras Functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "595af23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized batch: (64, 300) | labels: (64,)\n"
     ]
    }
   ],
   "source": [
    "# 02_load_data & datasets — builds ds_val and ds_test\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "BATCH = 64\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# assumes DATA points to ../Final Data (from your earlier cell)\n",
    "train = pd.read_csv(DATA / \"train.csv\")\n",
    "val   = pd.read_csv(DATA / \"val.csv\")\n",
    "test  = pd.read_csv(DATA / \"test.csv\")\n",
    "\n",
    "def ds_from_df(df):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df[\"text\"].values, df[\"fake\"].values.astype(\"int32\")))\n",
    "    ds = ds.batch(BATCH).map(lambda x, y: (vectorize_strings(x), y), num_parallel_calls=AUTOTUNE)\n",
    "    return ds.prefetch(AUTOTUNE)\n",
    "\n",
    "ds_val  = ds_from_df(val)\n",
    "ds_test = ds_from_df(test)\n",
    "\n",
    "# Quick peek (also asserts the pipeline works)\n",
    "ids_b, y_b = next(iter(ds_val.take(1)))\n",
    "print(\"Vectorized batch:\", ids_b.shape, \"| labels:\", y_b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e85d7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen threshold (F1-opt on val): 0.470\n"
     ]
    }
   ],
   "source": [
    "# 03_choose_threshold — sweep thresholds on val to maximize F1\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "val_proba = model.predict(ds_val, verbose=0).ravel()\n",
    "y_val = val[\"fake\"].values\n",
    "\n",
    "ths = np.linspace(0.05, 0.95, 181)\n",
    "best_thr, best_f1 = 0.5, -1.0\n",
    "for t in ths:\n",
    "    pred = (val_proba >= t).astype(int)\n",
    "    _, _, f1, _ = precision_recall_fscore_support(y_val, pred, average=\"binary\", zero_division=0)\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_thr = f1, t\n",
    "\n",
    "print(f\"Chosen threshold (F1-opt on val): {best_thr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b9fa916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"threshold\": 0.4699999999999999,\n",
      "  \"test_auc\": 0.9629379277978923,\n",
      "  \"accuracy\": 0.8999721370855391,\n",
      "  \"precision\": 0.9033412887828163,\n",
      "  \"recall\": 0.8848626534190532,\n",
      "  \"f1\": 0.8940064954236788\n",
      "}\n",
      "Saved: reports\\cnn\\test_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# 04_evaluate_on_test — AUC/Accuracy/Precision/Recall/F1 and save metrics JSON\n",
    "\n",
    "import json\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "test_proba = model.predict(ds_test, verbose=0).ravel()\n",
    "y_true = test[\"fake\"].values\n",
    "y_pred = (test_proba >= best_thr).astype(int)\n",
    "\n",
    "auc = roc_auc_score(y_true, test_proba)\n",
    "rep = classification_report(y_true, y_pred, digits=4, output_dict=True)\n",
    "\n",
    "metrics = {\n",
    "    \"threshold\": float(best_thr),\n",
    "    \"test_auc\": float(auc),\n",
    "    \"accuracy\": float(rep[\"accuracy\"]),\n",
    "    \"precision\": float(rep[\"1\"][\"precision\"]),\n",
    "    \"recall\": float(rep[\"1\"][\"recall\"]),\n",
    "    \"f1\": float(rep[\"1\"][\"f1-score\"]),\n",
    "    \"full_report\": rep,\n",
    "}\n",
    "\n",
    "(REPORT / \"test_metrics.json\").write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(json.dumps({k: metrics[k] for k in [\"threshold\", \"test_auc\", \"accuracy\", \"precision\", \"recall\", \"f1\"]}, indent=2))\n",
    "print(\"Saved:\", REPORT / \"test_metrics.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "866f702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: reports\\cnn\\confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "# 05_confusion_matrix — save PNG\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix  # if not already imported\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.title(\"Confusion Matrix (test)\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
    "plt.colorbar()\n",
    "\n",
    "for (i, j), v in np.ndenumerate(cm):\n",
    "    plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORT / \"confusion_matrix.png\", dpi=160)\n",
    "plt.close()\n",
    "print(\"Saved:\", REPORT / \"confusion_matrix.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5abbfaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: preds\\cnn\\test_preds.csv (3589 rows)\n"
     ]
    }
   ],
   "source": [
    "# 06_save_predictions — per-row predictions CSV\n",
    "\n",
    "preds_df = pd.DataFrame({\n",
    "    \"id\":   test[\"id\"],\n",
    "    \"text\": test[\"text\"],\n",
    "    \"y_true\": y_true,\n",
    "    \"proba\": test_proba,\n",
    "    \"y_pred\": y_pred\n",
    "})\n",
    "preds_df.to_csv(PRED/\"test_preds.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Saved:\", PRED/\"test_preds.csv\", f\"({len(preds_df)} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03c5f4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings -> embeddings\\cnn\n"
     ]
    }
   ],
   "source": [
    "# 07_export_embeddings — penultimate-layer features for clustering teammates\n",
    "\n",
    "# Penultimate layer (Dense(128) before sigmoid). Adjust index if you changed architecture.\n",
    "emb_model = keras.Model(model.input, model.layers[-2].output)\n",
    "\n",
    "def embed_texts(df):\n",
    "    ds = (tf.data.Dataset.from_tensor_slices(df[\"text\"].values)\n",
    "          .batch(BATCH)\n",
    "          .map(vectorize_strings, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "          .prefetch(tf.data.AUTOTUNE))\n",
    "    return emb_model.predict(ds, verbose=0)\n",
    "\n",
    "np.save(EMB/\"emb_train.npy\", embed_texts(train))\n",
    "np.save(EMB/\"emb_val.npy\",   embed_texts(val))\n",
    "np.save(EMB/\"emb_test.npy\",  embed_texts(test))\n",
    "print(\"Saved embeddings ->\", EMB)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
