{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: c:\\Users\\Clarice Shim\\Desktop\\COS30049 Computing Technology Innovation project\\COS30047_Session7_Group4\\SUBMISSION\\Models\\CNN\n",
      "TF: 2.19.0 | Keras: 3.11.3\n",
      "DATA: C:\\Users\\Clarice Shim\\Desktop\\COS30049 Computing Technology Innovation project\\COS30047_Session7_Group4\\SUBMISSION\\Final Data\n",
      "Will write to: {'MODELS': 'C:\\\\Users\\\\Clarice Shim\\\\Desktop\\\\COS30049 Computing Technology Innovation project\\\\COS30047_Session7_Group4\\\\SUBMISSION\\\\Models\\\\CNN\\\\models\\\\cnn', 'REPORT': 'C:\\\\Users\\\\Clarice Shim\\\\Desktop\\\\COS30049 Computing Technology Innovation project\\\\COS30047_Session7_Group4\\\\SUBMISSION\\\\Models\\\\CNN\\\\reports\\\\cnn', 'PRED': 'C:\\\\Users\\\\Clarice Shim\\\\Desktop\\\\COS30049 Computing Technology Innovation project\\\\COS30047_Session7_Group4\\\\SUBMISSION\\\\Models\\\\CNN\\\\preds\\\\cnn'}\n"
     ]
    }
   ],
   "source": [
    "# author: Clarice Shim\n",
    "# 00_setup (train)\n",
    "\n",
    "import os, random, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# quieter TF logs + reproducibility\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# ---- Paths ----\n",
    "MODEL_NAME = \"cnn\"\n",
    "\n",
    "# Notebook CWD = <repo>/Models/CNN\n",
    "# CSVs live     = <repo>/Final Data/*.csv   (go up two levels, then into Final Data)\n",
    "DATA   = Path(\"../..\") / \"Final Data\"\n",
    "\n",
    "# Outputs stay inside Models/CNN/\n",
    "MODELS = Path(\"models\") / MODEL_NAME        # Models/CNN/models/cnn/...\n",
    "REPORT = Path(\"reports\") / MODEL_NAME       # Models/CNN/reports/cnn/...\n",
    "PRED   = Path(\"preds\") / MODEL_NAME         # Models/CNN/preds/cnn/...\n",
    "\n",
    "for p in [MODELS, REPORT, PRED]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sanity checks (train needs train/val)\n",
    "assert (DATA / \"train.csv\").exists() and (DATA / \"val.csv\").exists(), \"Missing train/val CSVs. Fix DATA path.\"\n",
    "\n",
    "# ---- TensorFlow imports ----\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# (optional) force CPU if needed\n",
    "try:\n",
    "    tf.config.set_visible_devices([], \"GPU\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"TF:\", tf.__version__, \"| Keras:\", keras.__version__)\n",
    "print(\"DATA:\", (DATA).resolve())\n",
    "print(\"Will write to:\", {\"MODELS\": str(MODELS.resolve()), \"REPORT\": str(REPORT.resolve()), \"PRED\": str(PRED.resolve())})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11ede21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (12918, 3)  Val: (1436, 3)\n",
      "Train balance: {0: 0.5233, 1: 0.4767}\n",
      "Val   balance: {0: 0.5237, 1: 0.4763}\n"
     ]
    }
   ],
   "source": [
    "# 01_load_data\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(DATA / \"train.csv\", dtype={\"id\":\"string\",\"text\":\"string\",\"fake\":\"Int8\"})\n",
    "val   = pd.read_csv(DATA / \"val.csv\",   dtype={\"id\":\"string\",\"text\":\"string\",\"fake\":\"Int8\"})\n",
    "\n",
    "for df, name in [(train,\"train\"), (val,\"val\")]:\n",
    "    assert {\"id\",\"text\",\"fake\"}.issubset(df.columns), f\"Columns missing in {name}\"\n",
    "    assert df[\"fake\"].isna().sum()==0, f\"Missing labels in {name}\"\n",
    "    assert set(df[\"fake\"].unique()) <= {0,1}, f\"Unexpected labels in {name}: {df['fake'].unique()}\"\n",
    "\n",
    "train[\"fake\"] = train[\"fake\"].astype(\"uint8\")\n",
    "val[\"fake\"]   = val[\"fake\"].astype(\"uint8\")\n",
    "\n",
    "print(f\"Train: {train.shape}  Val: {val.shape}\")\n",
    "print(\"Train balance:\", train[\"fake\"].value_counts(normalize=True).round(4).to_dict())\n",
    "print(\"Val   balance:\", val[\"fake\"].value_counts(normalize=True).round(4).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed13250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer ready. Vocab size: 21087\n"
     ]
    }
   ],
   "source": [
    "# 02_text_vectorizer\n",
    "VOCAB_SIZE = 30_000\n",
    "SEQ_LEN    = 300\n",
    "BATCH      = 64\n",
    "\n",
    "text_vec = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQ_LEN,\n",
    "    standardize=\"lower_and_strip_punctuation\"\n",
    ")\n",
    "text_vec.adapt(train[\"text\"].values)  # fit on TRAIN ONLY\n",
    "\n",
    "# Save vocab (for reproducibility / rebuilding)\n",
    "vocab = text_vec.get_vocabulary()\n",
    "(MODELS / \"vocab.txt\").write_text(\"\\n\".join(vocab), encoding=\"utf-8\")\n",
    "print(\"Vectorizer ready. Vocab size:\", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d7ade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ids: (64, 300) labels: (64,)\n"
     ]
    }
   ],
   "source": [
    "# 03_build_datasets\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def make_ds(df, shuffle=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df[\"text\"].values, df[\"fake\"].values.astype(\"int32\")))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(len(df), reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(BATCH).map(lambda x,y: (text_vec(x), y), num_parallel_calls=AUTOTUNE)\n",
    "    return ds.prefetch(AUTOTUNE).cache()\n",
    "\n",
    "ds_train = make_ds(train, shuffle=True)\n",
    "ds_val   = make_ds(val, shuffle=False)\n",
    "\n",
    "x_b, y_b = next(iter(ds_train.take(1)))\n",
    "print(\"Batch ids:\", x_b.shape, \"labels:\", y_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1834c8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cnn_text_classifier\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"cnn_text_classifier\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">123,072</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gmp (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ drop (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ ids (\u001b[38;5;33mInputLayer\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m3,840,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv (\u001b[38;5;33mConv1D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │       \u001b[38;5;34m123,072\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gmp (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ drop (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ out (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,987,905</span> (15.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,987,905\u001b[0m (15.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,987,905</span> (15.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,987,905\u001b[0m (15.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 04_define_model\n",
    "def build_cnn(vocab_size=VOCAB_SIZE, seq_len=SEQ_LEN, emb_dim=128, filters=192, kernel=5, dense_units=128, dropout=0.3):\n",
    "    inp = layers.Input(shape=(seq_len,), dtype=\"int32\", name=\"ids\")\n",
    "    x = layers.Embedding(vocab_size, emb_dim, name=\"embedding\")(inp)\n",
    "    x = layers.Conv1D(filters, kernel, padding=\"same\", activation=\"relu\", name=\"conv\")(x)\n",
    "    x = layers.GlobalMaxPooling1D(name=\"gmp\")(x)\n",
    "    x = layers.Dropout(dropout, name=\"drop\")(x)\n",
    "    x = layers.Dense(dense_units, activation=\"relu\", name=\"dense\")(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\", name=\"out\")(x)\n",
    "    return keras.Model(inp, out, name=\"cnn_text_classifier\")\n",
    "\n",
    "model = build_cnn()\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[keras.metrics.AUC(name=\"auc\"), \"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "154c2e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks ready. Class weights: {0: 0.955473372781065, 1: 1.048879506333225}\n"
     ]
    }
   ],
   "source": [
    "# 06_callbacks_and_weights  (run this BEFORE the fit cell)\n",
    "from tensorflow import keras\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# ── Callbacks ─────────────────────────────────────────────────────────────────\n",
    "ckpt = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(MODELS / \"cnn_best.keras\"),\n",
    "    monitor=\"val_auc\",        # must match the metric name you use\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    verbose=1,\n",
    ")\n",
    "early = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_auc\", mode=\"max\", patience=2, restore_best_weights=True\n",
    ")\n",
    "reduce = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=1, min_lr=1e-6, verbose=1\n",
    ")\n",
    "csvlog = keras.callbacks.CSVLogger(str(REPORT / \"train_log.csv\"))\n",
    "\n",
    "callbacks = [ckpt, early, reduce, csvlog]\n",
    "\n",
    "# ── Class weights (optional; comment out if you don't want weighting) ────────\n",
    "# If you have the training labels in a DataFrame called `train`\n",
    "# (e.g., created earlier via: train = pd.read_csv(DATA/'train.csv'))\n",
    "y_train = train[\"fake\"].values.astype(int)\n",
    "\n",
    "w = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array([0, 1]),\n",
    "    y=y_train,\n",
    ")\n",
    "class_weight = {0: float(w[0]), 1: float(w[1])}\n",
    "\n",
    "print(\"Callbacks ready. Class weights:\", class_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eddaa4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7229 - auc: 0.8077 - loss: 0.5206\n",
      "Epoch 1: val_auc improved from None to 0.95917, saving model to models\\cnn\\cnn_best.keras\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 124ms/step - accuracy: 0.8159 - auc: 0.9057 - loss: 0.3912 - val_accuracy: 0.9004 - val_auc: 0.9592 - val_loss: 0.2586 - learning_rate: 0.0010\n",
      "Epoch 2/12\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9275 - auc: 0.9786 - loss: 0.1830\n",
      "Epoch 2: val_auc improved from 0.95917 to 0.96142, saving model to models\\cnn\\cnn_best.keras\n",
      "\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 115ms/step - accuracy: 0.9501 - auc: 0.9887 - loss: 0.1342 - val_accuracy: 0.8969 - val_auc: 0.9614 - val_loss: 0.2878 - learning_rate: 0.0010\n",
      "Epoch 3/12\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9836 - auc: 0.9981 - loss: 0.0524\n",
      "Epoch 3: val_auc did not improve from 0.96142\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 119ms/step - accuracy: 0.9905 - auc: 0.9992 - loss: 0.0331 - val_accuracy: 0.8990 - val_auc: 0.9596 - val_loss: 0.3267 - learning_rate: 5.0000e-04\n",
      "Epoch 4/12\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9955 - auc: 0.9999 - loss: 0.0147\n",
      "Epoch 4: val_auc did not improve from 0.96142\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - accuracy: 0.9976 - auc: 1.0000 - loss: 0.0093 - val_accuracy: 0.8935 - val_auc: 0.9574 - val_loss: 0.3462 - learning_rate: 2.5000e-04\n",
      "{'best_val_auc': 0.961421549320221, 'best_val_acc': 0.9004178047180176}\n"
     ]
    }
   ],
   "source": [
    "# 07_train\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    epochs=12,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight,   # or set to None if you don't want weighting\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "best_val_auc = float(max(history.history.get(\"val_auc\", [0.0])))\n",
    "best_val_acc = float(max(history.history.get(\"val_accuracy\", [0.0])))\n",
    "print({\"best_val_auc\": best_val_auc, \"best_val_acc\": best_val_acc})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75099a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Clarice Shim\\Desktop\\COS30018 Intelligent Systems\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "INFO:tensorflow:Assets written to: models\\cnn\\text_vectorizer_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\cnn\\text_vectorizer_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: models\\cnn\\cnn_final.keras | vec dir: models\\cnn\\text_vectorizer_model | metadata.json\n"
     ]
    }
   ],
   "source": [
    "# 08_save_artifacts\n",
    "# Save final model (best already saved by checkpoint)\n",
    "final_path = MODELS / \"cnn_final.keras\"\n",
    "model.save(final_path)\n",
    "\n",
    "# Save the TextVectorization as a Keras model and export using SavedModel dir\n",
    "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "squeezed = layers.Lambda(lambda t: tf.squeeze(t, axis=1))(string_input)\n",
    "vec_out = text_vec(squeezed)\n",
    "vec_model = keras.Model(string_input, vec_out, name=\"text_vectorizer_model\")\n",
    "vec_dir = MODELS / \"text_vectorizer_model\"\n",
    "tf.saved_model.save(vec_model, str(vec_dir))  # directory format (Windows-safe)\n",
    "\n",
    "# Metadata\n",
    "import json\n",
    "metadata = {\n",
    "    \"seed\": SEED,\n",
    "    \"vocab_size\": len(text_vec.get_vocabulary()),\n",
    "    \"seq_len\": int(SEQ_LEN),\n",
    "    \"batch\": int(BATCH),\n",
    "    \"best_val_auc\": best_val_auc,\n",
    "    \"best_val_acc\": best_val_acc,\n",
    "    \"paths\": {\n",
    "        \"best\": str((MODELS / \"cnn_best.keras\").resolve()),\n",
    "        \"final\": str(final_path.resolve()),\n",
    "        \"vectorizer_model_dir\": str(vec_dir.resolve()),\n",
    "        \"vocab_txt\": str((MODELS / \"vocab.txt\").resolve())\n",
    "    }\n",
    "}\n",
    "(MODELS / \"metadata.json\").write_text(json.dumps(metadata, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved:\", final_path, \"| vec dir:\", vec_dir, \"| metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c387bd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plots: ['reports\\\\cnn\\\\curve_loss.png', 'reports\\\\cnn\\\\curve_accuracy.png', 'reports\\\\cnn\\\\curve_auc.png']\n"
     ]
    }
   ],
   "source": [
    "# 09_plot_learning_curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(h, key, title=None):\n",
    "    plt.figure()\n",
    "    plt.plot(h.history.get(key, []), label=f\"train_{key}\")\n",
    "    val_key = f\"val_{key}\"\n",
    "    if val_key in h.history:\n",
    "        plt.plot(h.history[val_key], label=val_key)\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(key)\n",
    "    if title: plt.title(title)\n",
    "    plt.legend(); plt.grid(True)\n",
    "    out = REPORT / f\"curve_{key}.png\"\n",
    "    plt.savefig(out, dpi=160, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return out\n",
    "\n",
    "outs = []\n",
    "for k in [\"loss\", \"accuracy\", \"auc\"]:\n",
    "    try:\n",
    "        outs.append(str(plot_history(history, k, f\"Training curves: {k}\")))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot {k}: {e}\")\n",
    "\n",
    "print(\"Saved plots:\", outs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
